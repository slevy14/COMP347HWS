{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2"
      ],
      "metadata": {
        "id": "GXmqHl_K3YMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### IMPORTS\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "cwFlW9J977u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K62Awyvl3TzP",
        "outputId": "cca91236-a54c-49a1-d95a-85f889d65d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0490821765437979\n"
          ]
        }
      ],
      "source": [
        "def RegressionAtHome(X_train, Y_train, x_pred, tau):\n",
        "\n",
        "    weights = []\n",
        "\n",
        "    # class_weight\n",
        "    # {class_label : weight}\n",
        "\n",
        "    # what is m in the cost function???\n",
        "    # use class_weights to incorporate weights into class function\n",
        "    # but what is m??? and will this actually work?\n",
        "    log_reg = LogisticRegression()\n",
        "    log_reg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    # modify class_weight = weights\n",
        "\n",
        "    pass #FIXME\n",
        "\n",
        "\n",
        "# Gradient is first partial derivatives of the cost function\n",
        "# Hessian Matrix is second partial derivatives of the cost function\n",
        "# Newton's method uses gradient and hessian for,,,, something\n",
        "\n",
        "\n",
        "# J function -- this is what we're trying to optimize\n",
        "# do we actually need this?\n",
        "def J(lamb, theta, w_i, y_i, x_i, m):  # can't call it lambda bc python\n",
        "    sum = 0\n",
        "    for j in range(m):\n",
        "        sum += w_i * (y_i * math.log(h(theta, x_i)) + (1 - y_i) * math.log(1 - h(theta, x_i)))\n",
        "    return ((-1 * lamb * magnitude(theta)**2) / 2) + sum\n",
        "\n",
        "\n",
        "# calculate the sigmoid function for x_i\n",
        "def h(theta, x_i):\n",
        "    # sigmoid function\n",
        "    x_i = np.array(x_i)\n",
        "    t = np.matmul(x_i.T, theta)  # if we're using the transpose of x_i, is this basically just a dot product? does this give a scalar?\n",
        "    val = 1 / (1 - math.exp(-1 * t))\n",
        "    # return 1 or 0 based on calculated value?\n",
        "    if val > 0.5:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# calculate gradient of J (w.r.t theta)\n",
        "def gradient_J(X, w_i, y_i, x_i, lamb, theta):\n",
        "    gradient = np.matmul(X.T, w_i * (y_i - h(theta, x_i))) - lamb * theta\n",
        "    return gradient\n",
        "\n",
        "\n",
        "# Hessian Matrix???\n",
        "def hessian_matrix(X, D, lamb):\n",
        "    hessian = np.matmul(np.matmul(X.T, D), X) - lamb * np.shape(D)[0]\n",
        "    return hessian\n",
        "\n",
        "\n",
        "# Diagonal Matrix????????\n",
        "def diagonal(w_i, x_i, theta):\n",
        "    D_ii = -1 * w_i * h(x_i, theta) * (1 - h(x_i, theta))\n",
        "    return D_ii\n",
        "\n",
        "\n",
        "# calculate the magnitude of a vector\n",
        "def magnitude(vector):\n",
        "    mag = np.linalg.norm(vector)\n",
        "    return mag\n",
        "\n",
        "# calculate the weight based on point x_q\n",
        "def calc_weight(x_q, x_i, tau):\n",
        "    x_q = np.array(x_q)\n",
        "    x_i = np.array(x_i)\n",
        "    difference = x_q - x_i\n",
        "    w_i = math.exp((magnitude(difference)**2) / (2 * tau**2))\n",
        "    return w_i\n",
        "\n",
        "\n",
        "##### PRINTS DOWN HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2"
      ],
      "metadata": {
        "id": "8ie4ZCHk4yPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Soft-Margin Linear SVM, C=.02 corresponds to graph #4. This is a linear model with larger margins, so the C value is lower.\n",
        "2. Soft-Margin Linear SVM, C=20 corresponds to graph #3. This is a linear model with smaller margins, so the C value is higher.\n",
        "3. A hard-margin kernel SVM with $k(u,v) = uv - (uv)^2$ corresponds to graph #5. This is a polynomial kernel, which matches the shape of this decision boundary.\n",
        "4. A hard-margin kernel SVM with $k(u,v) = exp(-5\\|u-v\\|^2)$ corresponds to graph #6. The high gamma value (5) implies that the decision boundary will be overfitted.\n",
        "5. A hard margin kernel SVM with $k(u,v) = exp(-\\frac{1}{5}\\|u-v\\|^2)$ corresponds to graph #1. The relatively low gamma value of $\\frac{1}{5}$ implies that the decision boundary will not be overfitted.\n"
      ],
      "metadata": {
        "id": "YkZeGtQD402u"
      }
    }
  ]
}